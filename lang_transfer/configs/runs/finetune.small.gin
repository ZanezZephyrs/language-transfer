from __gin__ import dynamic_registration
import __main__ as train_script

import optax

from t5x import utils
from t5x import trainer

from lang_transfer import tasks

include "t5x/configs/runs/pretrain.gin"
include "lang_transfer/configs/models/70M.gin"

# REQUIRED variables
EVAL_PERIOD = %gin.REQUIRED

# PREDEFINED variables
BATCH_SIZE = 512
MICRO_BATCHES = 4
TASK_FEATURE_LENGTHS={"targets":1024}
EVAL_STEPS = 600 # to run the validation over aprox 315789473 tokens 
PRETRAINED_MODEL_PATH = []  # From scratch

DROPOUT_RATE = 0.0

# TRAIN script overrides
train_script.train.eval_steps = %EVAL_STEPS
train_script.train.eval_period = %EVAL_PERIOD

# MICRO-BATCHES & LR SCHEDULER configuration
trainer.Trainer:
  num_microbatches = %MICRO_BATCHES
  learning_rate_fn = @optax.warmup_cosine_decay_schedule()

optax.warmup_cosine_decay_schedule:
  init_value = 0.0
  peak_value = 2e-4
  end_value = 2e-5
  warmup_steps = %WARMUP_STEPS
  decay_steps = %TRAIN_STEPS

# PRETRAINED selection
utils.RestoreCheckpointConfig:
    path = %PRETRAINED_MODEL_PATH
    mode = 'specific'
    dtype = 'float32'

utils.SaveCheckpointConfig:
  period = %EVAL_PERIOD
